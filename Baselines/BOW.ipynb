{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quickumls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_240273/594625782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mquickumls\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuickUMLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'quickumls'"
     ]
    }
   ],
   "source": [
    "from quickumls import QuickUMLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data_stats_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('/home/shan/Desktop/netlab/Esophagitis/Proper_split_data/train.csv')\n",
    "dev = pd.read_csv('/home/shan/Desktop/netlab/Esophagitis/Proper_split_data/dev.csv')\n",
    "test = pd.read_csv('/home/shan/Desktop/netlab/Esophagitis/Proper_split_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1243 entries, 0 to 1242\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   PMRN             1243 non-null   int64  \n",
      " 1   Free Text Grade  1243 non-null   float64\n",
      " 2   Full Text        1243 non-null   object \n",
      " 3   TxDose           1243 non-null   float64\n",
      " 4   TxFx             1243 non-null   int64  \n",
      " 5   esophv55         1243 non-null   float64\n",
      " 6   esophmean        1243 non-null   float64\n",
      " 7   technique        1243 non-null   object \n",
      " 8   frequency        1243 non-null   object \n",
      " 9   ih               1243 non-null   object \n",
      " 10  ap               1243 non-null   object \n",
      " 11  sec_text         1243 non-null   object \n",
      " 12  exam             1243 non-null   object \n",
      " 13  rot              1243 non-null   object \n",
      " 14  ros              1243 non-null   object \n",
      " 15  TumorEQD2        1243 non-null   float64\n",
      " 16  Age              1243 non-null   int64  \n",
      " 17  Gender           1243 non-null   object \n",
      "dtypes: float64(5), int64(3), object(10)\n",
      "memory usage: 174.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([dev, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'esophagitis' in train['Full Text'][0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_check_g = []\n",
    "train_check_tf = []\n",
    "for grade, text in zip(train['Free Text Grade'], train['Full Text']):\n",
    "\tc = 'esophagitis' in text.lower()\n",
    "\tif c:\n",
    "\t\tc = 1\n",
    "\telse:\n",
    "\t\tc = 0\n",
    "\ttrain_check_g.append(grade)\n",
    "\ttrain_check_tf.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_tt = []\n",
    "for grade, text in zip(test['Free Text Grade'], test['Full Text']):\n",
    "\tc = str('esophagitis' in text.lower())\n",
    "\ttest_check_tt.append(str(grade)+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_check_tt = []\n",
    "for grade, text in zip(train['Free Text Grade'], train['Full Text']):\n",
    "\tc = str('esophagitis' in text.lower())\n",
    "\ttrain_check_tt.append(str(grade)+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'2.0False': 9,\n",
       "         '1.0True': 138,\n",
       "         '1.0False': 8,\n",
       "         '2.0True': 127,\n",
       "         '0.0False': 550,\n",
       "         '0.0True': 319,\n",
       "         '3.0True': 74,\n",
       "         '3.0False': 18})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train_check_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1.0True': 52,\n",
       "         '1.0False': 9,\n",
       "         '2.0True': 48,\n",
       "         '3.0True': 8,\n",
       "         '0.0True': 73,\n",
       "         '0.0False': 88,\n",
       "         '2.0False': 3})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_check_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_g = []\n",
    "test_check_tf = []\n",
    "for grade, text in zip(test['Free Text Grade'], test['Full Text']):\n",
    "\tc = 'esophagitis' in text.lower()\n",
    "\tif c:\n",
    "\t\tc = 1\n",
    "\telse:\n",
    "\t\tc = 0\n",
    "\ttest_check_g.append(grade)\n",
    "\ttest_check_tf.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_check = pd.DataFrame({\n",
    "\t'grade':train_check_g,\n",
    "\t'Eso_is_in_text':train_check_tf\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check = pd.DataFrame({\n",
    "\t'grade':test_check_g,\n",
    "\t'Eso_is_in_text':test_check_tf\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_check.to_csv('/home/shan/Desktop/netlab/Esophagitis/baselines/stats/t_check.csv', index=False)\n",
    "test_check.to_csv('/home/shan/Desktop/netlab/Esophagitis/baselines/stats/test_check.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing, extracting entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = '/home/shan/Desktop/netlab/Esophagitis/aug_data_v3/'\n",
    "PATH2 = '/home/shan/Desktop/netlab/Esophagitis/Proper_split_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(PATH1+'train.csv')\n",
    "dev = pd.read_csv(PATH1+'dev.csv')\n",
    "test = pd.read_csv(PATH1+'test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH2+'train.csv')\n",
    "dev = pd.read_csv(PATH2+'dev.csv')\n",
    "test = pd.read_csv(PATH2+'test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entities(df):\n",
    "\t'''\n",
    "\tinput a dataframe and add a column of extracted entities from the pipeline\n",
    "\t'''\n",
    "\tresult = []\n",
    "\tfor i in df['Full Text']:\n",
    "\t\tresult.append(' '.join([ent['word'] for ent in pipe(i)]))\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference time about 11mins\n",
    "train_r = add_entities(train)\n",
    "dev_r = add_entities(dev)\n",
    "test_r = add_entities(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ents'] = train_r\n",
    "dev['ents'] = dev_r\n",
    "test['ents'] = test_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([dev, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using augmented data\n"
     ]
    }
   ],
   "source": [
    "#change labeling name\n",
    "try:\n",
    "\ttrain['degree'] = train['Free Text Grade']\n",
    "\ttest['degree'] = test['Free Text Grade']\n",
    "except:\n",
    "\tprint('using augmented data, do not need to do this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding binary class labels\n",
    "train['binary'] = [1 if int(i)>0 else 0 for i in train['degree']]\n",
    "test['binary'] = [1 if int(i)>0 else 0 for i in test['degree']] \n",
    "\n",
    "train['severe'] = [1 if int(i)>1 else 0 for i in train['degree']]\n",
    "test['severe'] = [1 if int(i)>1 else 0 for i in test['degree']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1, BoW on full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1243, 11465)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "  \n",
    "count_vect = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(train['Full Text'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.98      0.71       237\n",
      "           1       0.38      0.02      0.03       188\n",
      "\n",
      "    accuracy                           0.55       425\n",
      "   macro avg       0.47      0.50      0.37       425\n",
      "weighted avg       0.48      0.55      0.41       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "text_clf.fit(train['Full Text'], train.binary)\n",
    "predicted = text_clf.predict(test['Full Text'])\n",
    "print(metrics.classification_report(test.binary, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       237\n",
      "           1       0.87      0.69      0.77       188\n",
      "\n",
      "    accuracy                           0.81       425\n",
      "   macro avg       0.83      0.80      0.81       425\n",
      "weighted avg       0.82      0.81      0.81       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "text_clf.fit(train['Full Text'], train.binary)\n",
    "predicted = text_clf.predict(test['Full Text'])\n",
    "print(metrics.classification_report(test.binary, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90       335\n",
      "           1       0.83      0.22      0.35        90\n",
      "\n",
      "    accuracy                           0.83       425\n",
      "   macro avg       0.83      0.61      0.63       425\n",
      "weighted avg       0.83      0.83      0.78       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "text_clf.fit(train['Full Text'], train.severe)\n",
    "predicted = text_clf.predict(test['Full Text'])\n",
    "print(metrics.classification_report(test.severe, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline2 BoW on entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.97      0.79       237\n",
      "           1       0.90      0.40      0.55       188\n",
      "\n",
      "    accuracy                           0.72       425\n",
      "   macro avg       0.79      0.68      0.67       425\n",
      "weighted avg       0.77      0.72      0.69       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "text_clf.fit(train['ents'], train.binary)\n",
    "predicted = text_clf.predict(test['ents'])\n",
    "print(metrics.classification_report(test.binary, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88       335\n",
      "           1       0.56      0.06      0.10        90\n",
      "\n",
      "    accuracy                           0.79       425\n",
      "   macro avg       0.68      0.52      0.49       425\n",
      "weighted avg       0.74      0.79      0.72       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "text_clf.fit(train['ents'], train.severe)\n",
    "predicted = text_clf.predict(test['ents'])\n",
    "print(metrics.classification_report(test.severe, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82       237\n",
      "           1       0.89      0.55      0.68       188\n",
      "\n",
      "    accuracy                           0.77       425\n",
      "   macro avg       0.81      0.75      0.75       425\n",
      "weighted avg       0.80      0.77      0.76       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "text_clf.fit(train['ap'], train.binary)\n",
    "predicted = text_clf.predict(test['ap'])\n",
    "print(metrics.classification_report(test.binary, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.91       335\n",
      "           1       0.89      0.27      0.41        90\n",
      "\n",
      "    accuracy                           0.84       425\n",
      "   macro avg       0.86      0.63      0.66       425\n",
      "weighted avg       0.85      0.84      0.80       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "text_clf.fit(train['ap'], train.severe)\n",
    "predicted = text_clf.predict(test['ap'])\n",
    "print(metrics.classification_report(test.severe, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('l8': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2253b22703bbd4e3705962a869a5b4519ff7329eae6d79c444dc1c5756bade73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
